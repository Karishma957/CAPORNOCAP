# Docker:

How do you normally run a node project, first you do node install and then you run npm start.
You will also have to mention the port somewhere as well. Now for more complex project, you will have more instructions, steps, more variables.

If you have to run this project on some other device, you will have to do all the installations step by step, run all the commands.

Docker makes this easier for you, it created a DockerFile which hae all these commands mentioned step wise in a file. Bonus it also has cache for each step( in care you have not added anything in package .json so running node install is pointless, so it skips that step).
And all you need is one command to run the project: `docker run yourProject`

Image is the list of instructions nad commands and variables, Container is a running instance of this image.

# Docker compose:

Now in complex projects, you won't just have node, you will also have kafka or database connected. Now we can create different container for each service and just create a `dockor-compose.yml` which will have list of all services we will use and volume( how the data is shared between these services) in it.
Docker compose used to orchestrate multi-containers.

# Zookeeper:

It is a distributed coordination service. Let's say we have 2 databases which store user data, one is in India and one in USA. Why do we need 2? if we write to single database from all geographic location it will be of terrible latency so we keep 2. Now we write to only on database , but we will have to keep the databases in sync. So we can achieve this consistency using zookeeper.

# Kafka:
Distributed streaming platform
Producer: Source of data, pushing records into kafka topic
Consumer: Consume data and perform proper action on it( eg store in db or update db)
Broker: Server stores data, can have multiple brokers in a cluster.
eache cluster consist of multiple borkers and a zookeeper for syncronization.
--------------
|   Cluster  |
|------------|  
|  Zookeeper |
|------------|
|   Broker1  |
|   Broker2  |
-------------- 

Topics: we can create multiple topics for different functionality like ordering, shipping, order returns data. We can create different partitions inside a topic ( Partition based on region or date)

Model folder holds JPA entities i.r Java classes mapped to database tables, Jakarta Persistence API (JPA).
@Entity marks a class as a JPA entity, SPring boot + Hibernate will create the table automatically when you run your app

@Data: Generates getters, setters, toString, equals, hashCode

@GeneratedValue(strategy = GenerationType.IDENTITY)
Auto-generates the ID value using your database's auto-increment feature.

@Repository: marks interface as a spring managed bean so that it can be automatically injected
public interface UserEventRepository extends JpaRepository<UserEvent(JPA Entity, maps with database table), Long(primary key type)> {}

@RequestBody	Deserializes JSON into Java object

@Configuartion: marks class as spring configuration class, it will be picked up automatically at startup to apply custom configurations like security settings

CSRF (Cross-Site Request Forgery) protection is useful for browser-based apps that use cookies.

# psql CLI open:
docker exec -it postgres psql -U userpulse -d postgres
\l

# Maven build
./mvnw spring-boot:run

# Pageable
    List<PlayerScore> findTopPlayers(Pageable pageable);
Here pageable requests a subset of data
Pageable topTen = PageRequest.of(0, 10); // page 0, size 10
List<PlayerScore> topPlayers = playerScoreRepository.findTopPlayers(topTen);

# mysql -u root
# docker-compose build --no-cache backend
# docker-compose up